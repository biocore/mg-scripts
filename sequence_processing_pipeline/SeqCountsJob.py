from collections import defaultdict
from .Job import Job, KISSLoader
from .PipelineError import JobFailedError
from glob import glob
from jinja2 import Environment
from metapool import load_sample_sheet
from os import walk
from os.path import join, split
import logging
import pandas as pd
import re


logging.basicConfig(level=logging.DEBUG)


class SeqCountsJob(Job):
    def __init__(self, run_dir, output_path, queue_name,
                 node_count, wall_time_limit, jmem, modules_to_load,
                 qiita_job_id, max_array_length, files_to_count_path,
                 sample_sheet_path, cores_per_task=4):
        """
        ConvertJob provides a convenient way to run bcl-convert or bcl2fastq
        on a directory BCL files to generate Fastq files.
        :param run_dir: The 'run' directory that contains BCL files.
        :param output_path: Path where all pipeline-generated files live.
        :param queue_name: The name of the Torque queue to use for processing.
        :param node_count: The number of nodes to request.
        :param wall_time_limit: A hard time limit (in min) to bound processing.
        :param jmem: String representing total memory limit for entire job.
        :param modules_to_load: A list of Linux module names to load
        :param qiita_job_id: identify Torque jobs using qiita_job_id
        :param max_array_length: A hard-limit for array-sizes
        :param files_to_count_path: A path to a list of file-paths to count.
        :param sample_sheet_path: A path to the sample-sheet.
        :param cores_per_task: (Optional) # of CPU cores per node to request.
        """
        super().__init__(run_dir,
                         output_path,
                         'SeqCountsJob',
                         [],
                         max_array_length,
                         modules_to_load=modules_to_load)

        self.queue_name = queue_name
        self.node_count = node_count
        self.wall_time_limit = wall_time_limit
        self.cores_per_task = cores_per_task

        # raise an Error if jmem is not a valid floating point value.
        self.jmem = str(int(jmem))
        self.qiita_job_id = qiita_job_id
        self.jinja_env = Environment(loader=KISSLoader('templates'))

        self.job_name = (f"seq_counts_{self.qiita_job_id}")
        self.files_to_count_path = files_to_count_path
        self.sample_sheet_path = sample_sheet_path

        with open(self.files_to_count_path, 'r') as f:
            lines = f.readlines()
            lines = [x.strip() for x in lines]
            lines = [x for x in lines if x != '']
            self.file_count = len(lines)

    def run(self, callback=None):
        job_script_path = self._generate_job_script()
        params = ['--parsable',
                  f'-J {self.job_name}',
                  f'--array 1-{self.file_count}']
        try:
            self.job_info = self.submit_job(job_script_path,
                                            job_parameters=' '.join(params),
                                            exec_from=None,
                                            callback=callback)

            logging.debug(f'SeqCountsJob Job Info: {self.job_info}')
        except JobFailedError as e:
            # When a job has failed, parse the logs generated by this specific
            # job to return a more descriptive message to the user.
            info = self.parse_logs()
            # prepend just the message component of the Error.
            info.insert(0, str(e))
            raise JobFailedError('\n'.join(info))

        self._aggregate_counts(self.sample_sheet_path)

        logging.debug(f'SeqCountJob {self.job_info["job_id"]} completed')

    def _generate_job_script(self):
        job_script_path = join(self.output_path, "seq_counts.sbatch")
        template = self.jinja_env.get_template("seq_counts.sbatch")

        #  got to make files_to_count.txt and put it in the output directory

        with open(job_script_path, mode="w", encoding="utf-8") as f:
            f.write(template.render({
                "job_name": "seq_counts",
                "wall_time_limit": self.wall_time_limit,
                "mem_in_gb": self.jmem,
                "node_count": self.node_count,
                "cores_per_task": self.cores_per_task,
                "queue_name": self.queue_name,
                "file_count": self.file_count,
                "files_to_count_path": self.files_to_count_path,
                "output_path": self.output_path
            }))

        return job_script_path

    def parse_logs(self):
        # overrides Job.parse_logs() w/tailored parse for specific logs.
        files = sorted(glob(join(self.log_path, '*.err')))
        msgs = []

        for some_file in files:
            with open(some_file, 'r') as f:
                msgs += [line for line in f.readlines()
                         if line.startswith("[E::stk_size]")]

        return [msg.strip() for msg in msgs]

    def _aggregate_counts_by_file(self):
        # aggregates sequence & bp counts from a directory of log files.

        def extract_metadata(log_output_file_path):
            """
            extracts sequence & bp counts from individual log files.
            """
            with open(log_output_file_path, 'r') as f:
                lines = f.readlines()
                lines = [x.strip() for x in lines]
                if len(lines) != 2:
                    raise ValueError(
                        "error processing %s" % log_output_file_path)
                _dir, _file = split(lines[0])
                seq_counts, base_pairs = lines[1].split('\t')
                return _dir, _file, int(seq_counts), int(base_pairs)

        results = defaultdict(dict)

        for root, dirs, files in walk(self.log_path):
            for _file in files:
                if _file.endswith('.out'):
                    log_output_file = join(root, _file)
                    _dir, _file, seq_counts, base_pairs = \
                        extract_metadata(log_output_file)

                    results[_file] = {
                        'seq_counts': seq_counts,
                        'base_pairs': base_pairs
                    }

        return results

    def _aggregate_counts(self, sample_sheet_path):
        """
        Aggregate results by sample_ids and write to file.
        Args:
            sample_sheet_path:

        Returns: None
        """
        def get_metadata(sample_sheet_path):
            sheet = load_sample_sheet(sample_sheet_path)

            lanes = []

            if sheet.validate_and_scrub_sample_sheet():
                results = {}

                for sample in sheet.samples:
                    barcode_id = sample['barcode_id']
                    sample_id = sample['Sample_ID']
                    lanes.append(sample['Lane'])
                    results[barcode_id] = sample_id

                lanes = list(set(lanes))

                if len(lanes) != 1:
                    raise ValueError(
                        "More than one lane is declared in sample-sheet")

                return results, lanes[0]

        mapping, lane = get_metadata(sample_sheet_path)

        # aggregate results by filename
        by_files = self._aggregate_counts_by_file()

        # aggregate results by sample. This means aggregating metadata
        # across two file names.
        counts = defaultdict(dict)

        for _file in by_files:
            if 'erroneous' in _file:
                # for now, don't include sequences in erroneous files as part
                # of the base counts.
                continue

            if re.match(r'TellReadJob_I1_C\d\d\d\.fastq.gz.corrected.err_'
                        r'barcode_removed.fastq', _file):
                # skip indexed read files. We only want counts from R1 and R2
                # files.
                continue

            m = re.match(r'TellReadJob_(R\d)_(C\d\d\d)\.fastq.gz.corrected.'
                         r'err_barcode_removed.fastq', _file)

            if not m:
                # if the filename doesn't match this pattern then unpredicted
                # behavior has occured.
                raise ValueError(
                    "'%s' doesnt appear to be a valid file name." % _file)

            orientation = m.group(1)
            barcode_id = m.group(2)

            sample_id = mapping[barcode_id]

            counts[sample_id][orientation] = by_files[_file]['seq_counts']

        sample_ids = []
        read_counts = []
        lanes = []

        for sample_id in counts:
            sample_ids.append(sample_id)
            read_counts.append(counts[sample_id]['R1'] +
                               counts[sample_id]['R2'])
            lanes.append(lane)

        df = pd.DataFrame(data={'Sample_ID': sample_ids,
                                'raw_reads_r1r2': read_counts,
                                'Lane': lanes})

        df.set_index(['Sample_ID', 'Lane'], verify_integrity=True)

        # sort results into a predictable order for testing purposes
        df = df.sort_values(by='Sample_ID')

        result_path = join(self.output_path, 'SeqCounts.csv')
        df.to_csv(result_path, index=False, sep=",")

        return result_path

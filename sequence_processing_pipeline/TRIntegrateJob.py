from os.path import join
from .Job import Job, KISSLoader
from .PipelineError import JobFailedError
import logging
from jinja2 import Environment
from .Pipeline import Pipeline
from .PipelineError import PipelineError
from metapool import load_sample_sheet
from os import makedirs
from shutil import copyfile


logging.basicConfig(level=logging.DEBUG)


class TRIntegrateJob(Job):
    def __init__(self, run_dir, output_path, sample_sheet_path, queue_name,
                 node_count, wall_time_limit, jmem, modules_to_load,
                 qiita_job_id, integrate_script_path, sil_path, raw_fastq_dir,
                 reference_base, reference_map, cores_per_task):
        """
        ConvertJob provides a convenient way to run bcl-convert or bcl2fastq
        on a directory BCL files to generate Fastq files.
        :param run_dir: The 'run' directory that contains BCL files.
        :param output_path: Path where all pipeline-generated files live.
        :param sample_sheet_path: The path to a sample-sheet.
        :param queue_name: The name of the Torque queue to use for processing.
        :param node_count: The number of nodes to request.
        :param wall_time_limit: A hard time limit (in min) to bound processing.
        :param jmem: String representing total memory limit for entire job.
        :param modules_to_load: A list of Linux module names to load
        :param qiita_job_id: identify Torque jobs using qiita_job_id
        :param integrate_script_path: None
        :param sil_path: A path to a confidential file mapping C5xx, adapters.
        :param reference_base: None
        :param reference_map: None
        :param cores_per_task: # of CPU cores per node to request.
        """
        super().__init__(run_dir,
                         output_path,
                         'TRIntegrateJob',
                         [],
                         # max_array_length and self.max_array_length are
                         # not used by TRIntegrateJob.
                         -1,
                         modules_to_load=modules_to_load)

        self.sample_sheet_path = sample_sheet_path
        self._file_check(self.sample_sheet_path)
        metadata = self._process_sample_sheet()
        self.sample_ids = metadata['sample_ids']
        self.queue_name = queue_name
        self.node_count = node_count
        self.wall_time_limit = wall_time_limit
        self.cores_per_task = cores_per_task
        self.integrate_script_path = integrate_script_path
        self.sil_path = sil_path
        self.raw_fastq_dir = raw_fastq_dir
        self.tmp_dir = join(self.output_path, 'tmp')

        self.reference_base = reference_base
        self.reference_map = reference_map

        # raise an Error if jmem is not a valid floating point value.
        self.jmem = str(int(jmem))
        self.qiita_job_id = qiita_job_id
        self.sample_count = len(self.sample_ids)
        self.jinja_env = Environment(loader=KISSLoader('templates'))
        self.job_name = (f"integrate_{self.qiita_job_id}")

        with open(self.sil_path, 'r') as f:
            # obtain the number of unique barcode_ids as determined by
            # TellReadJob() in order to set up an array job of the
            # proper length.
            lines = f.readlines()
            lines = [x.strip() for x in lines]
            lines = [x for x in lines if x != '']
            self.barcode_id_count = len(lines)

    def run(self, callback=None):
        job_script_path = self._generate_job_script()

        # copy sil_path to TRIntegrate working directory and rename to a
        # predictable name.
        copyfile(self.sil_path,
                 join(self.output_path, 'sample_index_list.txt'))

        # generate the tailored subset of adapter to barcode_id based on
        # the proprietary lists owned by the manufacturer and supplied by
        # the caller, and the barcode ids found in the sample-sheet.
        self._generate_sample_index_list()

        makedirs(self.tmp_dir)

        params = ['--parsable',
                  f'-J {self.job_name}',
                  f'--array 1-{self.sample_count}']
        try:
            self.job_info = self.submit_job(job_script_path,
                                            job_parameters=' '.join(params),
                                            exec_from=None,
                                            callback=callback)

            logging.debug(f'TRIntegrateJob Job Info: {self.job_info}')
        except JobFailedError as e:
            # When a job has failed, parse the logs generated by this specific
            # job to return a more descriptive message to the user.
            info = self.parse_logs()
            # prepend just the message component of the Error.
            info.insert(0, str(e))
            raise JobFailedError('\n'.join(info))

        logging.debug(f'TRIntegrateJob {self.job_info["job_id"]} completed')

    def _process_sample_sheet(self):
        sheet = load_sample_sheet(self.sample_sheet_path)

        if not sheet.validate_and_scrub_sample_sheet():
            s = "Sample sheet %s is not valid." % self.sample_sheet_path
            raise PipelineError(s)

        header = sheet.Header
        chemistry = header['chemistry']

        if header['Assay'] not in Pipeline.assay_types:
            s = "Assay value '%s' is not recognized." % header['Assay']
            raise PipelineError(s)

        sample_ids = []
        for sample in sheet.samples:
            sample_ids.append((sample['Sample_ID'], sample['Sample_Project']))

        bioinformatics = sheet.Bioinformatics

        # reorganize the data into a list of dictionaries, one for each row.
        # the ordering of the rows will be preserved in the order of the list.
        lst = bioinformatics.to_dict('records')

        # human-filtering jobs are scoped by project. Each job requires
        # particular knowledge of the project.
        return {'chemistry': chemistry,
                'projects': lst,
                'sample_ids': sample_ids}

    def _generate_job_script(self):
        job_script_path = join(self.output_path, 'integrate_test.sbatch')
        template = self.jinja_env.get_template("integrate.sbatch")

        with open(job_script_path, mode="w", encoding="utf-8") as f:
            f.write(template.render({
                "job_name": "integrate",
                "wall_time_limit": self.wall_time_limit,
                "mem_in_gb": self.jmem,
                "node_count": self.node_count,
                "cores_per_task": self.cores_per_task,
                "integrate_script_path": self.integrate_script_path,
                "queue_name": self.queue_name,
                "barcode_id_count": self.barcode_id_count,
                "raw_fastq_dir": self.raw_fastq_dir,
                "tmp_dir": self.tmp_dir,
                "output_dir": self.output_path}))

        return job_script_path

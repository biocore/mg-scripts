{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of args:  3\n",
      "Arg list:  ['/Users/jdereus/miniconda3/envs/seq_proc/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/jdereus/Library/Jupyter/runtime/kernel-d3b2badc-87bc-490f-8800-e3877905046c.json']\n",
      "/Users/jdereus/seq_proc_dev/bcl_human_template.sh\n",
      "2021-08-20\n",
      "/Users/jdereus/Desktop/seq_proc_dev/\n",
      "current working dir /Users/jdereus/Desktop/seq_proc_dev\n",
      "08/20/2021 :: 16:10:35  --> /Users/jdereus/Desktop/seq_proc_dev//sequencing_test\n",
      "sequencing_test exists\n",
      "new dirs values ['sequencing_test']\n",
      "new data path = /Users/jdereus/Desktop/seq_proc_dev/sequencing_test\n",
      "RTAComplete but no sample sheets\n",
      "data is processing for /Users/jdereus/Desktop/seq_proc_dev/sequencing_test\n",
      "directory =  sequencing_test\n",
      "dir= sequencing_test\n",
      "some text sequencing_test\n",
      "data path =  /Users/jdereus/Desktop/seq_proc_dev/sequencing_test\n",
      "sample sheets found : amplicon_sample.csv\n",
      "bio_index= 18\n",
      "Assay TruSeq HT\n",
      "Chemistry Amplicon\n",
      "ReverseComplement 0\n",
      "Amplicon chemistry true. Removing false barcodes\n",
      "--use-bases-mask Y150,I12,I12,Y150\n",
      "Assay TruSeq HT\n",
      "Chemistry Amplicon\n",
      "ReverseComplement 0\n",
      "contact_index ==  26\n",
      "False\n",
      "after empty df check\n",
      "                   0               1               2              3  \\\n",
      "22  [Bioinformatics]               .               .              .   \n",
      "24           Project  ForwardAdapter  ReverseAdapter  PolyGTrimming   \n",
      "26      apple_test_1               .               .          FALSE   \n",
      "28      apple_test_1               .               .          FALSE   \n",
      "30      apple_test_1               .               .          FALSE   \n",
      "32      apple_test_1               .               .          FALSE   \n",
      "34      apple_test_1               .               .          FALSE   \n",
      "\n",
      "                 4        5  6  7  8  9  \n",
      "22               .        .  .  .  .  .  \n",
      "24  HumanFiltering  QiitaID  .  .  .  .  \n",
      "26           FALSE    10556  .  .  .  .  \n",
      "28           FALSE    10567  .  .  .  .  \n",
      "30           FALSE    10568  .  .  .  .  \n",
      "32           FALSE    10569  .  .  .  .  \n",
      "34           FALSE    10570  .  .  .  .  \n",
      "                      0\n",
      "0  some_email@gmail.com\n",
      "        9    10   11\n",
      "0  [Reads]  151  151\n",
      "1        .    .    .\n",
      "2        .    .    .\n",
      "3        .    .    .\n",
      "4        .    .    .\n",
      "5        .    .    .\n",
      "6        .    .    .\n",
      "7        .    .    .\n",
      "8        .    .    .\n",
      "9        .    .    .\n",
      "          0  1  2  3  4  5  6  7  8  9\n",
      "9   [Reads]  .  .  .  .  .  .  .  .  .\n",
      "10      151  .  .  .  .  .  .  .  .  .\n",
      "11      151  .  .  .  .  .  .  .  .  .\n",
      "sample sheets found : metasample.csv\n",
      "experiment name = RKL0072\n",
      "bio_index= 27\n",
      "contact_index= 30\n",
      "ExperimentName RKL0072\n",
      "Assay Metagenomics\n",
      "Chemistry Default\n",
      "ReverseComplement 0\n",
      "ExperimentName RKL0072\n",
      "Assay Metagenomics\n",
      "Chemistry Default\n",
      "ReverseComplement 0\n",
      "contact_index ==  35\n",
      "False\n",
      "after empty df check\n",
      "                   0                                  1   \\\n",
      "28   [Bioinformatics]                                  .   \n",
      "30       project_name                     ForwardAdapter   \n",
      "31         iseq_44444  GATCGGAAGAGCACACGTCTGAACTCCAGTCAC   \n",
      "33          [Contact}                                  .   \n",
      "34   jdereus@ucsd.edu                                  .   \n",
      "35  ackermag@ucsd.edu                                  .   \n",
      "36  mmbryant@ucsd.edu                                  .   \n",
      "\n",
      "                                   2              3               4        5   \\\n",
      "28                                  .              .               .        .   \n",
      "30                     ReverseAdapter  PolyGTrimming  HumanFiltering  QiitaID   \n",
      "31  GATCGGAAGAGCGTCGTGTAGGGAAAGGAGTGT           TRUE           FALSE    44444   \n",
      "33                                  .              .               .        .   \n",
      "34                                  .              .               .        .   \n",
      "35                                  .              .               .        .   \n",
      "36                                  .              .               .        .   \n",
      "\n",
      "   6  7  8  9  10  \n",
      "28  .  .  .  .  .  \n",
      "30  .  .  .  .  .  \n",
      "31  .  .  .  .  .  \n",
      "33  .  .  .  .  .  \n",
      "34  .  .  .  .  .  \n",
      "35  .  .  .  .  .  \n",
      "36  .  .  .  .  .  \n",
      "                      0\n",
      "0  some_email@gmail.com\n",
      "         13   14   15\n",
      "0   [Reads]  150  150\n",
      "1         .    .    .\n",
      "2         .    .    .\n",
      "3         .    .    .\n",
      "4         .    .    .\n",
      "5         .    .    .\n",
      "6         .    .    .\n",
      "7         .    .    .\n",
      "8         .    .    .\n",
      "9         .    .    .\n",
      "10        .    .    .\n",
      "         0  1  2  3  4  5  6  7  8  9  10\n",
      "13  [Reads]  .  .  .  .  .  .  .  .  .  .\n",
      "14      150  .  .  .  .  .  .  .  .  .  .\n",
      "15      150  .  .  .  .  .  .  .  .  .  .\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os, sys, time, shutil, re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from time import time as _time\n",
    "import glob\n",
    "import smtplib, ssl\n",
    "from smtplib import SMTP_SSL as smtp\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "print('Number of args: ', len(sys.argv))\n",
    "print('Arg list: ', str(sys.argv))\n",
    "\n",
    "NPROCS=16\n",
    "bcl_human_template=os.path.expanduser('~') + \"/seq_proc_dev/bcl_human_template.sh\"\n",
    "print(bcl_human_template)\n",
    "\n",
    "#scan_dir will normally be provided by cron job scanning this location\n",
    "scan_dir = '/Users/jdereus/Desktop/seq_proc_dev/'\n",
    "\n",
    "data_path = scan_dir\n",
    "\n",
    "# RTAComplete.txt is final file written by sequencers\n",
    "check_file = \"RTAComplete.txt\"\n",
    "\n",
    "def get_new_directories(scan_dir):\n",
    "    \"\"\"Get any new sequencing raw data locations\"\"\"\n",
    "\n",
    "    new_dirs = []\n",
    "    today = date.today()\n",
    "    print(today)\n",
    "    print(scan_dir)\n",
    "\n",
    "    ### allow for 1 day to scan back in time\n",
    "    os.chdir(scan_dir)\n",
    "    print(\"current working dir \" + os.getcwd())\n",
    "    for root, dirs, files in os.walk(scan_dir):\n",
    "        for name in dirs:\n",
    "            filedate = date.fromtimestamp(os.path.getmtime(os.path.join(root,name)))\n",
    "            if (today - filedate).days < 1:\n",
    "                new_dirs.append(name)\n",
    "        break\n",
    "\n",
    "    #print(new_dirs)\n",
    "\n",
    "    for dir in new_dirs:\n",
    "        timestamp_str = time.strftime ( '%m/%d/%Y :: %H:%M:%S',\n",
    "                                       time.gmtime(os.path.getmtime(scan_dir + '/' + dir)))\n",
    "        print(timestamp_str, ' -->', (scan_dir + '/' + dir))\n",
    "\n",
    "    #print(\"found \" + str(len(new_dirs)) + \" new directories\")\n",
    "\n",
    "    for dir in (new_dirs):\n",
    "        #print(dir)\n",
    "        if os.path.isfile(os.path.join(scan_dir, dir, \"RTAComplete.txt\")):\n",
    "            print(dir + \" exists\")\n",
    "\n",
    "    return new_dirs\n",
    "\n",
    "def prep_data_location(run_directory, scan_dir):\n",
    "    data_path = os.path.join(scan_dir,run_directory)\n",
    "    mk_path = os.path.join(data_path, \"Data/Fastq\")\n",
    "\n",
    "    print(\"new data path = \" + data_path)\n",
    "    if (os.path.isfile(os.path.join(data_path, \"RTAComplete.txt\"))) and (os.path.isfile(os.path.join(data_path, \"*.csv\"))):\n",
    "        (\"###continue\")\n",
    "    elif (os.path.isfile(os.path.join(data_path, \"RTAComplete.txt\"))) and (not os.path.isfile(os.path.join(data_path, \"*.csv\"))):\n",
    "        print(\"RTAComplete but no sample sheets\")\n",
    "        smtp = smtplib.SMTP()\n",
    "        ###smtp.connect(\"localhost\", 25)\n",
    "        ###smtp.sendmail(from_addr=\"test_email@noreply.com\", to_addrs=\"jdereus@gmail.com\", msg=\"something is wrong\")\n",
    "\n",
    "    if os.path.isfile(os.path.join(data_path, \"alockfile\")):\n",
    "        print(\"data is processing for \" + data_path)\n",
    "    elif os.path.isfile(os.path.join(data_path, \"processed\")):\n",
    "        print(\"data processing is complete \" + data_path)\n",
    "    elif (not os.path.exists(os.path.join(data_path, \"alockfile\"))) or (not os.path.exists(os.path.join(data_path, \"processed\"))):\n",
    "        try:\n",
    "            Path(os.path.join(data_path, \"alockfile\")).touch()\n",
    "        except not FileExistsError:\n",
    "            print(\"unable to get lockfile\")\n",
    "\n",
    "    if not os.path.isfile(os.path.join(data_path, \"alockfile\")):\n",
    "        try:\n",
    "                ### make data output location for bcl conversion\n",
    "            ###mk_path = os.path.join(data_path, \"Data/Fastq\")\n",
    "            os.makedirs(mk_path, mode=750, exist_ok=True)\n",
    "        except OSError as error:\n",
    "            print(error)\n",
    "\n",
    "    return mk_path\n",
    "\n",
    "def parse_csv(directory):\n",
    "    print(\"inside test_csv \")\n",
    "    os.chdir(os.path.join(scan_dir, directory))\n",
    "    for csvfile in glob.glob('*.{}'.format(\"csv\")):\n",
    "        print(csvfile)\n",
    "        sections = [lines.index(line) for line in csvfile if \"[\" in line]\n",
    "        print(line)\n",
    "        print(sections)\n",
    "        for i in range(len(sections)):\n",
    "            header = lines[sections[i]]\n",
    "            df = pd.DataFrame(lines[sections[i]+1:sections[i+1]],\n",
    "                              columns=header)\n",
    "            print(\"head items \" + df.head())\n",
    "\n",
    "\n",
    "\n",
    "def process_data(directory, data_path, mk_path):\n",
    "    contact_df = pd.DataFrame({'':[]})\n",
    "    read_df = pd.DataFrame({'':[]})\n",
    "    bioinfo_df = pd.DataFrame({'':[]})\n",
    "    #info_dict = []\n",
    "    csvfile=''\n",
    "    base_mask=''\n",
    "    bclconvert_template=''\n",
    "    dependent_job=False\n",
    "    experiment_name=''\n",
    "\n",
    "    extension = 'csv'\n",
    "    print(\"directory = \", directory)\n",
    "    print(\"dir=\",directory)\n",
    "    os.chdir(os.path.join(scan_dir, directory))\n",
    "    print(\"some text \" + directory)\n",
    "\n",
    "    samplesheet_copies = \"orig_sample_sheets\"\n",
    "\n",
    "    data_path = os.path.join(data_path, directory)\n",
    "\n",
    "    print(\"data path = \", data_path)\n",
    "    ### create directory for original sample sheets\n",
    "    sample_sheet_storage=os.path.join(data_path, samplesheet_copies)\n",
    "    if not os.path.exists(sample_sheet_storage):\n",
    "        os.mkdir(sample_sheet_storage)\n",
    "\n",
    "    for csvfile in glob.glob('*.{}'.format(extension)):\n",
    "        N_count=0\n",
    "        job_index_val=\"I12\"\n",
    "\n",
    "        #with open(csvfile, 'r') as csvfile:\n",
    "        print(\"sample sheets found : \" + csvfile)\n",
    "        df = pd.read_csv(csvfile, header=None)#,\n",
    "                        #na_values=['NaN'])\n",
    "\n",
    "        ### copy orig sample sheet to separate directory.  this does NOT create numerical increments of file copies\n",
    "        ### one copy only of latest file\n",
    "        sample_sheet = os.path.join(data_path, csvfile)\n",
    "        if not os.path.isfile(os.path.join(sample_sheet_storage, csvfile+\".bak\")):\n",
    "            shutil.copyfile(sample_sheet, os.path.join(sample_sheet_storage, csvfile+\".bak\"))\n",
    "            #df.head()\n",
    "\n",
    "        # drop all lines with no data (all \"NaN\")\n",
    "        ### effectively compresses sample sheet to no blank lines.\n",
    "        df = df.dropna(how='all')\n",
    "\n",
    "        ### search for Bioinformatics does not work unless we replace NaN with .\n",
    "        ### TODO: figure this out\n",
    "        newdf = df.fillna(\".\")\n",
    "\n",
    "        info_df = pd.DataFrame()\n",
    "\n",
    "        ### initialize dictionary for sample sheet info\n",
    "        ### experiment, chemistry, etc.\n",
    "        info_dict = {}\n",
    "\n",
    "        ### initialize contact_index in case no contact information on sample sheet.\n",
    "        contact_index=int()\n",
    "        ### intialize bio_index in case no bioinformatics information\n",
    "        bio_index=int()\n",
    "\n",
    "        for index in range(len(newdf)):\n",
    "            if 'Bioinformatics' in newdf.iloc[index,0]: ##,0]:\n",
    "                bio_index = index\n",
    "                #### bio_index=i ###newdf.iloc[i,0]\n",
    "                print(\"bio_index=\", index)\n",
    "            if '[Contact' in newdf.iloc[index,0]: ##,0]:\n",
    "                print(\"contact_index=\", index)\n",
    "                ### remove [Contact] label.  reference maillist by index == 0\n",
    "                contact_index = index+1 #newdf.iloc[i,0]\n",
    "            else:\n",
    "                contact_index=len(newdf)\n",
    "            if '[Reads]' in newdf.iloc[index,0]:\n",
    "                read_index_1 = index\n",
    "                read_index_2 = index+3\n",
    "\n",
    "            ### assemble dictionary for csv variables\n",
    "            if 'Experiment' in newdf.iloc[index,0]:\n",
    "                info_df = info_df.append(newdf.iloc[index])\n",
    "                experiment_name = newdf.iloc[index,1]\n",
    "                info_dict[\"ExperimentName\"] = experiment_name\n",
    "                print(\"experiment name =\", experiment_name)\n",
    "            if 'Assay' in newdf.iloc[index,0]:\n",
    "                info_df = info_df.append(newdf.iloc[index])\n",
    "                assay_type = newdf.iloc[index,1]\n",
    "                info_dict['Assay'] = assay_type\n",
    "            if 'Chemistry' in newdf.iloc[index,0]:\n",
    "                info_df = info_df.append(newdf.iloc[index])\n",
    "                chemistry_type = newdf.iloc[index,1]\n",
    "                info_dict['Chemistry'] = chemistry_type\n",
    "            if 'ReverseComplement' in newdf.iloc[index,0]:\n",
    "                info_df = info_df.append(newdf.iloc[index])\n",
    "                reverse_comp = newdf.iloc[index,1]\n",
    "                info_dict['ReverseComplement'] = reverse_comp\n",
    "\n",
    "            #print(info_df.T)\n",
    "        for key,value in info_dict.items():\n",
    "            print(key,value)\n",
    "\n",
    "        if info_dict[\"Chemistry\"] == \"Amplicon\":\n",
    "            print(\"Amplicon chemistry true. Removing false barcodes\")\n",
    "        #else: print(\"False\")\n",
    "            NNN_list=[\"NNNNNNNN\", \"NNNNNNNNNNNN\"]\n",
    "            for line in csvfile:\n",
    "                for entry in NNN_list:\n",
    "                    if entry in line:\n",
    "                        print(N_count=len(entry))\n",
    "\n",
    "            if N_count == \"12\":\n",
    "                job_index_val = \"I12\"\n",
    "            elif N_count == \"8\":\n",
    "                job_index_val=\"I8\"\n",
    "            elif N_count == 0:\n",
    "                job_index_val=\"I12\"\n",
    "\n",
    "            base_mask=\"--use-bases-mask Y150\" + \",\" + job_index_val + \",\" + job_index_val +\",\" + \"Y150\"\n",
    "            print(base_mask)\n",
    "\n",
    "            ### check to see if both Read values are present at 150/151\n",
    "            ### if both, direction==2\n",
    "            ### else direction==1\n",
    "            ### used for bases-mask in bcl2fastq\n",
    "            ### pull from read_df or develop other method\n",
    "\n",
    "\n",
    "            ### replace N strings if Amplicon Sample sheets with blank for processing.\n",
    "            ### can be moved to where we parse chemistry variable.\n",
    "            ### we have already copied original to backup location.\n",
    "            ### create new sample sheet with same name as original, removing false barcodes\n",
    "            csv_1 = open(csvfile, 'r')\n",
    "\n",
    "\n",
    "            ### should check to see if NNNNNNNN is present but this is historical\n",
    "            csv_1 = ''.join([i for i in csv_1]).replace(\"NNNNNNNNNNNN\", \"\")\n",
    "            csv_2 = open(csvfile, 'w')\n",
    "            csv_2.writelines(csv_1)\n",
    "            csv_2.close()\n",
    "\n",
    "            #for line in csvfile:\n",
    "            #    for entry in NNN_list:\n",
    "            #        if entry in line:\n",
    "            #            line = line.replace(entry,\"\")\n",
    "            #            print(N_count=len(entry))\n",
    "\n",
    "        for key,value in info_dict.items():\n",
    "            print(key,value)\n",
    "\n",
    "\n",
    "        bioinfo_df=pd.DataFrame(newdf.iloc[bio_index:contact_index-1])\n",
    "        #bioinfo_df=bioinfo_df.dropna()\n",
    "        #header = bioinfo_df.iloc[]\n",
    "        #bioinfo_df.columns = header\n",
    "        bioinfo_df\n",
    "\n",
    "        print(\"contact_index == \", contact_index)\n",
    "\n",
    "        contact_df=pd.DataFrame(newdf.iloc[contact_index:])\n",
    "        if contact_df.empty:\n",
    "            contact_df = pd.DataFrame({\"some_email@gmail.com\"})\n",
    "        print(contact_df.empty)\n",
    "\n",
    "\n",
    "        print(\"after empty df check\")\n",
    "        ### access read count by column index == 0\n",
    "        read_df=pd.DataFrame(newdf.iloc[read_index_1:read_index_2])\n",
    "        #read_df.columns = [\"Reads\"]\n",
    "        #read_df = pd.DataFrame(read_df.reshape(3,-1))\n",
    "\n",
    "        print(bioinfo_df)\n",
    "        print(contact_df)\n",
    "\n",
    "        print(read_df.T)\n",
    "        #read_df = read_df.T\n",
    "        read_df = read_df.dropna(how='all')\n",
    "        print(read_df)\n",
    "\n",
    "    ### job submission building\n",
    "    ### mk_path contains fastq output location.\n",
    "\n",
    "    if os.path.isdir(mk_path):\n",
    "        print(\"directory exists\", mk_path)\n",
    "\n",
    "    sbatch_submit(mk_path, contact_df, read_df, bioinfo_df, info_dict, csvfile, base_mask, bclconvert_template, dependent_job, experiment_name)\n",
    "\n",
    "\n",
    "def sbatch_submit(mk_path, contact_df, read_df, bioinfo_df, info_dict, csvfile, base_mask, bclconvert_template, dependent_job, experiment_name ):\n",
    "    scl_cmd_proc = []\n",
    "    \"\"\" need to work on job script output format for below write/append contact_index\n",
    "    #!/bin/bash\n",
    "    #SBATCH --job-name=RKL0072\n",
    "    #SBATCH --ntasks=16\n",
    "    #SBATCH --ntasks-per-node=16\n",
    "    #SBATCH --export=ALL\n",
    "    #SBATCH --time=72:00:00\n",
    "    #SBATCH --mem-per-cpu=6G\n",
    "    #SBATCH --output=_output_path/%x_%A_%a.out\n",
    "    #SBATCH --error=output_path/%x_%A_%a.err\n",
    "    #SBATCH --mail-type=e,TIME_LIMIT_80\n",
    "    #SBATCH --mail-user=someemail@gmail.com\n",
    "    source ~/.bash_profile\n",
    "    ['sbatch ', '--parsable ', '--qos=seq_proc ', '--export=seqdir=<user_location>/seq_proc_dev,outputdir=,csvfile=metasample.csv,base_mask=--use-bases-mask Y150,I12,I12,Y150 ', '--job-name=RKL0072 ', ' ']\n",
    "    \"\"\"\n",
    "\n",
    "    job_file = open(os.path.join(mk_path, experiment_name + \"_sbatch.sh\"), 'a')\n",
    "    job_file.write(\"#!/bin/bash\\n\")\n",
    "    job_file.write(\"#SBATCH --job-name=%s\\n\" % experiment_name)\n",
    "    job_file.write(\"#SBATCH --ntasks=16\\n\")\n",
    "    job_file.write(\"#SBATCH --ntasks-per-node=16\\n\")\n",
    "    job_file.write(\"#SBATCH --export=ALL\\n\")\n",
    "    job_file.write(\"#SBATCH --time=72:00:00\\n\")\n",
    "    job_file.write(\"#SBATCH --mem-per-cpu=6G\\n\")\n",
    "    job_file.write(\"#SBATCH --output=_output_path/%x_%A_%a.out\\n\")\n",
    "    job_file.write(\"#SBATCH --error=output_path/%x_%A_%a.err\\n\")\n",
    "    job_file.write(\"#SBATCH --mail-type=e,TIME_LIMIT_80\\n\")\n",
    "    job_file.write(\"#SBATCH --mail-user=someemail@gmail.com\\n\")\n",
    "\n",
    "    job_file.write(\"source ~/.bash_profile\\n\")\n",
    "    scl_cmd_proc.append('sbatch ')\n",
    "\n",
    "    if dependent_job:\n",
    "        scl_cmd_proc.append(\"--dependency=afterok:%s \" % dependency_job_id)\n",
    "\n",
    "    scl_cmd_proc.append(\"--parsable \")\n",
    "    scl_cmd_proc.append(\"--qos=seq_proc \")\n",
    "    scl_cmd_proc.append(\"--export=seqdir=%s,outputdir=%s,csvfile=%s,base_mask=%s \" % (data_path,mk_path,csvfile,base_mask))\n",
    "    scl_cmd_proc.append(\"--job-name=%s \" % experiment_name)\n",
    "    scl_cmd_proc.append(\"%s \" % bclconvert_template)\n",
    "\n",
    "    job_file.write(str(scl_cmd_proc))\n",
    "\n",
    "def human_filter():\n",
    "    for dir in new_directories:\n",
    "        print(dir)\n",
    "\n",
    "def run_param():\n",
    "    print(\"whatup\")\n",
    "\n",
    "\n",
    "new_directories = get_new_directories(scan_dir)\n",
    "print(\"new dirs values \" + str(new_directories))\n",
    "\n",
    "### loop over all new unprocessed directories\n",
    "###mk_path=str()\n",
    "\n",
    "for directory in new_directories:\n",
    "    prep_data_location(directory, scan_dir)\n",
    "    process_data(directory, data_path, mk_path)\n",
    "    ###parse_csv(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
